{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Algorithms\n",
    "\n",
    "* K-means\n",
    "* K-mediod\n",
    "* K-mode\n",
    "* DBSCAN\n",
    "* K-prototypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means Clustering Algorithm:\n",
    "* <img src=\"clustering.gif\" width=\"300\" height=\"300\" />\n",
    "* KMeans is also called centroid based or partitioning algorithm\n",
    "* KMeans class is part of `sklearn.cluster.Kmeans`\n",
    "\n",
    "\n",
    "* `class sklearn.cluster.KMeans(n_clusters=8, init=’k-means++’, n_init=10, max_iter=300, tol=0.0001,     precompute_distances=’auto’, verbose=0, random_state=None, copy_x=True, n_jobs=None, algorithm=’auto’)`\n",
    "\n",
    "\n",
    "* ***K-Means clustering parameters***:\n",
    "    * **n_clusters** : int, optional, default: 8\n",
    "        The number of clusters to form as well as the number of centroids to generate.\n",
    "\n",
    "    * **init** : {‘k-means++’, ‘random’ or an ndarray}\n",
    "      Method for initialization, *defaults to ‘k-means++’*:\n",
    "      *Manual ndarray* :It should be of shape (n_clusters, n_features) and gives the initial centers.\n",
    "\n",
    "    * **n_init** : int, default: 10\n",
    "      Number of time the k-means algorithm will be run with different centroid seeds. best run will be choosen\n",
    "\n",
    "    * **max_iter** : int, default: 300\n",
    "      Maximum number of iterations of the k-means algorithm for a single run.\n",
    "\n",
    "    * **tol** : float, default: 1e-4\n",
    "      Relative tolerance with regards to inertia to declare convergence\n",
    "\n",
    "    * **precompute_distances** : {‘auto’, True, False}\n",
    "      Precompute distances (faster but takes more memory).\n",
    "\n",
    "      ‘auto’ : do not precompute distances if n_samples * n_clusters > 12 million. This corresponds to about 100MB        overhead per job using double precision.\n",
    "       True : always precompute distances\n",
    "       False : never precompute distances\n",
    "\n",
    "    * **verbose** : int, default 0\n",
    "      Verbosity mode.\n",
    "\n",
    "    * **random_state** : int, RandomState instance or None (default)\n",
    "      Determines random number generation for centroid initialization. Use an int to make the randomness                 deterministic. See Glossary.\n",
    "\n",
    "    *  **n_jobs** : int or None, optional (default=None)\n",
    "       The number of jobs to use for the computation. This works by computing each of the n_init runs in parallel.\n",
    " \n",
    "       None means 1 unless in a joblib.parallel_backend context. -1 means using all processors. See Glossary for            more details.\n",
    "\n",
    "    * **algorithm** : “auto”, “full” or “elkan”, default=”auto”\n",
    "      K-means algorithm to use. The classical EM-style algorithm is “full”. The “elkan” variation is more efficient      by using the triangle inequality, but currently doesn’t support sparse data. “auto” chooses “elkan” for dense       data and “full” for sparse data.\n",
    "      \n",
    "* ***Kmeans clustering Attributes***:\n",
    "    * cluster_centers_ : array, [n_clusters, n_features]\n",
    "       Coordinates of cluster centers. If the algorithm stops before fully converging (see tol and max_iter), these        will not be consistent with labels_.\n",
    "\n",
    "     * labels_ : Labels of each point\n",
    "\n",
    "     * inertia_ : float Sum of squared distances of samples to their closest cluster center.\n",
    "\n",
    "     *  n_iter_ : int Number of iterations run.\n",
    "     \n",
    "* ***kmeans Clustering Methods***:\n",
    "    * fit(self, X[, y, sample_weight])\tCompute k-means clustering.\n",
    "    * fit_predict(self, X[, y, sample_weight])\tCompute cluster centers and predict cluster index for each sample.\n",
    "    * fit_transform(self, X[, y, sample_weight])\tCompute clustering and transform X to cluster-distance space.\n",
    "    * get_params(self[, deep])\tGet parameters for this estimator.\n",
    "    * predict(self, X[, sample_weight])\tPredict the closest cluster each sample in X belongs to.\n",
    "    * score(self, X[, y, sample_weight])\tOpposite of the value of X on the K-means objective.\n",
    "    * set_params(self, \\*\\*params)\tSet the parameters of this estimator.\n",
    "    * transform(self, X)\tTransform X to a cluster-distance space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "ks = range(1, 6)\n",
    "inertias = []\n",
    "\n",
    "for k in ks:\n",
    "    # Create a KMeans instance with k clusters: model\n",
    "    model = KMeans(n_clusters=k)\n",
    "\n",
    "    # Fit model to samples\n",
    "    model.fit(samples)\n",
    "\n",
    "    # Append the inertia to the list of inertias\n",
    "    inertias.append(model.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize elbow method to identify possible k value\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "# Plot ks vs inertias\n",
    "plt.plot(ks, inertias, '-o')\n",
    "plt.xlabel('number of clusters, k')\n",
    "plt.ylabel('inertia')\n",
    "plt.xticks(ks)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantages:\n",
    "\n",
    "1. Fast, robust and easier to understand.\n",
    "2. Relatively efficient: O(tknd), where n is number of objects,\n",
    "   k is number of clusters, d is number of dimension of each\n",
    "   object, and t is number of iterations. Normally, k, t d < n.\n",
    "3. Gives best result when data set are distinct or well\n",
    "   separated from each other.\n",
    "   \n",
    "Disadvantages : \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting the number of clusters with silhouette analysis on KMeans clustering\n",
    "\n",
    "Silhouette analysis can be used to study the separation distance between the resulting clusters. The silhouette plot displays a measure of how close each point in one cluster is to points in the neighboring clusters and thus provides a way to assess parameters like number of clusters visually. This measure has a range of [-1, 1].\n",
    "\n",
    "`score = silhouette_score (df, preds, metric='euclidean')\n",
    "    print (\"For n_clusters = {}, silhouette score is {})\".format(n_clusters, score)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Mediod\n",
    "\n",
    "1. Initialize: select k random points out of the n data points as the medoids.\n",
    "2. Associate each data point to the closest medoid by using any common distance metric methods.\n",
    "3. While the cost decreases:\n",
    "        For each medoid m, for each data o point which is not a medoid:\n",
    "                1. Swap m and o, associate each data point to the closest medoid, recompute the cost.\n",
    "                2. If the total cost is more than that in the previous step, undo the swap.\n",
    "    \n",
    "    Its implementat in Sklean_extra `from sklearn_extra.cluster import KMedoids`\n",
    "\n",
    "\n",
    "* Parameters:\n",
    "    * n_clustersint, optional, default: 8\n",
    "      The number of clusters to form as well as the number of medoids to generate.\n",
    "    * init{‘random’, ‘heuristic’, ‘k-medoids++’}, optional, default: ‘heuristic’\n",
    "      Specify medoid initialization method. ‘random’ selects n_clusters elements from the dataset. ‘heuristic’ \n",
    "      picks the n_clusters points with the smallest sum distance to every other point. ‘k-medoids++’ follows\n",
    "      approach based on k-means++_, and in general, gives initial medoids which are more separated than those     \n",
    "      generated by the other methods.\n",
    "    * max_iterint, optional, default\n",
    "      Specify the maximum number of iterations when fitting.\n",
    "\n",
    "    * random_stateint, RandomState instance or None, optional\n",
    "        Specify random state for the random number generator. Used to initialise medoids when init=’random’.\n",
    "* Advantages:\n",
    "\n",
    "    1. It is simple to understand and easy to implement.\n",
    "    2. K-Medoid Algorithm is fast and converges in a fixed number of steps.\n",
    "    3. PAM is less sensitive to outliers than other partitioning algorithms.\n",
    "* Disadvantages:\n",
    "\n",
    "    1. The main disadvantage of K-Medoid algorithms is that it is not suitable for clustering non-spherical (arbitrary shaped) groups of objects. This is because it relies on minimizing the distances between the non-medoid objects and the medoid (the cluster center) – briefly, it uses compactness as clustering criteria instead of connectivity.\n",
    "   2. It may obtain different results for different runs on the same dataset because the first k medoids are chosen randomly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    `kmedoids = KMedoids(n_clusters=2, random_state=0).fit(X)\n",
    "     kmedoids.labels_\n",
    "     kmedoids.predict()\n",
    "     kmedoids.cluster_centers_\n",
    "     kmedoids.inertia_\n",
    "     `  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN (Density Based Spatial Clustering of Application with Noise):\n",
    "![Dbscan](dbscan.gif)\n",
    "\n",
    "* The main concept of DBSCAN algorithm is to locate high density that is sepearated from low density region\n",
    "    * Eps(epsilon) Density at a point P , Number of points within a cirlce of radius\n",
    "    * Minpts (Minpoints) Density Region , for each point in cluster circle with radius r contains atleast minimum number of points.\n",
    "    * Core point : In density region if it satisfies >=Minpts \n",
    "    * Border point : if Number of points in region < Minpts, but it lies in neighbhorhood of another core point.\n",
    "    * Noise : Neither core point or Border point\n",
    "<img src=\"dbs1.png\" width=\"300\" height=\"300\"/> \n",
    "\n",
    "\n",
    "\n",
    "[DBSCAN_PAPER](https://www.aaai.org/Papers/KDD/1996/KDD96-037.pdf \"DBSCAN Whitepaper\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps of DBSCAN Algorithm:\n",
    "\n",
    "* The algorithm starts with an arbitrary point which has not been visited and its neighborhood information is retrieved from the ϵ parameter.\n",
    "* If this point contains MinPts within ϵ neighborhood, cluster formation starts. Otherwise the point is labeled as noise. This point can be later found within the ϵ neighborhood of a different point and, thus can be made a part of the cluster. Concept of density reachable and density connected points are important here.\n",
    "* If a point is found to be a core point then the points within the ϵ neighborhood is also part of the cluster. So all the points found within ϵ neighborhood are added, along with their own ϵ neighborhood, if they are also core points.\n",
    "* The above process continues until the density-connected cluster is completely found.\n",
    "* The process restarts with a new point which can be a part of a new cluster or labeled as noise.\n",
    "\n",
    "## Drawbacks of DBSCAN algorithm.\n",
    "* If the database has data points that form clusters of varying density, then DBSCAN fails to cluster the data points well, since the clustering depends on ϵ and MinPts parameter, they cannot be chosen separately for all clusters.\n",
    "* If the data and features are not so well understood by a domain expert then, setting up ϵ and MinPts could be tricky and, may need comparisons for several iterations with different values of ϵ and MinPts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">   ```\n",
    "    from sklearn.cluster import DBSCAN\n",
    "    import sklearn.utils\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    weather_df_clus_temp = weather_df[[\"Tm\", \"Tx\", \"Tn\", \"xm\", \"ym\"]]\n",
    "    weather_df_clus_temp = StandardScaler().fit_transform(weather_df_clus_temp)\n",
    "    db = DBSCAN(eps=0.3, min_samples=10).fit(weather_df_clus_temp)\n",
    "    labels = db.labels_\n",
    "    print (labels[500:560])\n",
    "    weather_df[\"Clus_Db\"]=labels\n",
    "    realClusterNum=len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    clusterNum = len(set(labels))\n",
    "    ```\n",
    "    \n",
    "[Visualization](https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Generate sample data\n",
    "centers = [[1, 1], [-1, -1], [1, -1]]\n",
    "X, labels_true = make_blobs(n_samples=750, centers=centers, cluster_std=0.4,\n",
    "                            random_state=0)\n",
    "\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# #############################################################################\n",
    "# Compute DBSCAN\n",
    "db = DBSCAN(eps=0.3, min_samples=10).fit(X)\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "labels = db.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "print('Estimated number of noise points: %d' % n_noise_)\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels_true, labels))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels_true, labels))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels_true, labels))\n",
    "print(\"Adjusted Rand Index: %0.3f\"\n",
    "      % metrics.adjusted_rand_score(labels_true, labels))\n",
    "print(\"Adjusted Mutual Information: %0.3f\"\n",
    "      % metrics.adjusted_mutual_info_score(labels_true, labels))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(X, labels))\n",
    "\n",
    "# #############################################################################\n",
    "# Plot result\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Black removed and is used for noise instead.\n",
    "unique_labels = set(labels)\n",
    "colors = [plt.cm.Spectral(each)\n",
    "          for each in np.linspace(0, 1, len(unique_labels))]\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == -1:\n",
    "        # Black used for noise.\n",
    "        col = [0, 0, 0, 1]\n",
    "\n",
    "    class_member_mask = (labels == k)\n",
    "\n",
    "    xy = X[class_member_mask & core_samples_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
    "             markeredgecolor='k', markersize=14)\n",
    "\n",
    "    xy = X[class_member_mask & ~core_samples_mask]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
    "             markeredgecolor='k', markersize=6)\n",
    "\n",
    "plt.title('Estimated number of clusters: %d' % n_clusters_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-modes and K-prototypes\n",
    "\n",
    "k-modes is used for clustering categorical variables.\n",
    "It defines clusters based on the number of matching categories between data points\n",
    "Implemented are:\n",
    "k-modes [HUANG97] [HUANG98]\n",
    "k-modes with initialization based on density [CAO09]\n",
    "k-prototypes [HUANG97]\n",
    "\n",
    "K-modes and k prototypes will accept np.NaN value for catergorical variable but for numerical variable it should be populated.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "from kmodes.kprototypes import KPrototypes\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "data = np.c_[iris['data'], iris['target']]\n",
    "\n",
    "kp = KPrototypes(n_clusters=3, init='Huang', n_init=1, verbose=True)\n",
    "kp.fit_predict(data, categorical=[4])\n",
    "\n",
    "print(kp.cluster_centroids_)\n",
    "print(kp.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import numpy as np\n",
    "from kmodes.kmodes import KModes\n",
    "\n",
    "# reproduce results on small soybean data set\n",
    "x = np.genfromtxt('soybean.csv', dtype=int, delimiter=',')[:, :-1]\n",
    "y = np.genfromtxt('soybean.csv', dtype=str, delimiter=',', usecols=(35, ))\n",
    "\n",
    "kmodes_huang = KModes(n_clusters=4, init='Huang', verbose=1)\n",
    "kmodes_huang.fit(x)\n",
    "\n",
    "# Print cluster centroids of the trained model.\n",
    "print('k-modes (Huang) centroids:')\n",
    "print(kmodes_huang.cluster_centroids_)\n",
    "# Print training statistics\n",
    "print('Final training cost: {}'.format(kmodes_huang.cost_))\n",
    "print('Training iterations: {}'.format(kmodes_huang.n_iter_))\n",
    "\n",
    "kmodes_cao = KModes(n_clusters=4, init='Cao', verbose=1)\n",
    "kmodes_cao.fit(x)\n",
    "\n",
    "# Print cluster centroids of the trained model.\n",
    "print('k-modes (Cao) centroids:')\n",
    "print(kmodes_cao.cluster_centroids_)\n",
    "# Print training statistics\n",
    "print('Final training cost: {}'.format(kmodes_cao.cost_))\n",
    "print('Training iterations: {}'.format(kmodes_cao.n_iter_))\n",
    "\n",
    "print('Results tables:')\n",
    "for result in (kmodes_huang, kmodes_cao):\n",
    "    classtable = np.zeros((4, 4), dtype=int)\n",
    "    for ii, _ in enumerate(y):\n",
    "        classtable[int(y[ii][-1]) - 1, result.labels_[ii]] += 1\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"    | Cl. 1 | Cl. 2 | Cl. 3 | Cl. 4 |\")\n",
    "    print(\"----|-------|-------|-------|-------|\")\n",
    "    for ii in range(4):\n",
    "        prargs = tuple([ii + 1] + list(classtable[ii, :]))\n",
    "        print(\" D{0} |    {1:>2} |    {2:>2} |    {3:>2} |    {4:>2} |\".format(*prargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from kmodes.kprototypes import KPrototypes\n",
    "\n",
    "# stocks with their market caps, sectors and countries\n",
    "syms = np.genfromtxt('travel.csv', dtype=str, delimiter=',')[:, 1]\n",
    "X = np.genfromtxt('travel.csv', dtype=object, delimiter=',')[:, 2:]\n",
    "X[:, 0] = X[:, 0].astype(float)\n",
    "\n",
    "kproto = KPrototypes(n_clusters=4, init='Cao', verbose=2)\n",
    "clusters = kproto.fit_predict(X, categorical=[1, 2])\n",
    "\n",
    "# Print cluster centroids of the trained model.\n",
    "print(kproto.cluster_centroids_)\n",
    "# Print training statistics\n",
    "print(kproto.cost_)\n",
    "print(kproto.n_iter_)\n",
    "\n",
    "for s, c in zip(syms, clusters):\n",
    "    print(\"Symbol: {}, cluster:{}\".format(s, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import numpy as np\n",
    "from kmodes.kprototypes import KPrototypes\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "style.use(\"ggplot\")\n",
    "colors = ['b', 'orange', 'g', 'r', 'c', 'm', 'y', 'k', 'Brown', 'ForestGreen']\n",
    "#Data points with their publisher name,category score, category name, place name\n",
    "syms = np.genfromtxt('travel.csv', dtype=str, delimiter=',')[:, 1]\n",
    "X = np.genfromtxt('travel.csv', dtype=object, delimiter=',')[:, 2:]\n",
    "X[:, 0] = X[:, 0].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kproto = KPrototypes(n_clusters=15, init='Huang', verbose=2)\n",
    "clusters = kproto.fit_predict(X, categorical=[1, 2])\n",
    "# Print cluster centroids of the trained model.\n",
    "print(kproto.cluster_centroids_)\n",
    "# Print training statistics\n",
    "print(kproto.cost_)\n",
    "#print(kproto.n_iter_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s, c in zip(syms, clusters):\n",
    "    print(\"Result: {}, cluster:{}\".format(s, c))\n",
    "# Plot the results\n",
    "for i in set(kproto.labels_):\n",
    "    index = kproto.labels_ == i\n",
    "    plt.plot(X[index, 0], X[index, 1], 'o')\n",
    "    plt.suptitle('Data points categorized with category score', fontsize=18)\n",
    "    plt.xlabel('Category Score', fontsize=16)\n",
    "    plt.ylabel('Category Type', fontsize=16)\n",
    "plt.show()\n",
    "# Clustered result\n",
    "fig1, ax3 = plt.subplots()\n",
    "scatter = ax3.scatter(syms, clusters, c=clusters, s=50)\n",
    "ax3.set_xlabel('Data points')\n",
    "ax3.set_ylabel('Cluster')\n",
    "plt.colorbar(scatter)\n",
    "ax3.set_title('Data points classifed according to known centers')\n",
    "plt.show()\n",
    "result = zip(syms, kproto.labels_)\n",
    "sortedR = sorted(result, key=lambda x: x[1])\n",
    "print(sortedR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
